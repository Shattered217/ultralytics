"""
æ•°æ®æ³„æ¼æ£€æµ‹ (Data Leakage Detection)

è¯¥è„šæœ¬ä½¿ç”¨æ„ŸçŸ¥å“ˆå¸Œï¼ˆPerceptual Hashï¼‰ç®—æ³•æ£€æµ‹æ•°æ®é›†ä¸­çš„è¿‘é‡å¤å›¾åƒï¼Œ
é˜²æ­¢è®­ç»ƒé›†å’ŒéªŒè¯é›†/æµ‹è¯•é›†ä¹‹é—´çš„æ•°æ®æ³„æ¼ã€‚

åŠŸèƒ½ï¼š
1. è®¡ç®—æ¯å¼ å›¾ç‰‡çš„æ„ŸçŸ¥å“ˆå¸Œï¼ˆä½¿ç”¨ average hash æˆ– difference hashï¼‰
2. æ£€æµ‹ train ä¸ val/test ä¹‹é—´çš„è¿‘é‡å¤å›¾åƒ
3. ç”Ÿæˆæ³„æ¼æŠ¥å‘Š CSV æ–‡ä»¶
4. æä¾›å¤„ç†å»ºè®®

ä½¿ç”¨æ–¹æ³•:
    python scripts/check_leakage.py --dataset datasets/openparts
    python scripts/check_leakage.py --dataset datasets/selfparts
    python scripts/check_leakage.py --all  # æ£€æµ‹æ‰€æœ‰æ•°æ®é›†
    
ä¾èµ–:
    pip install pillow numpy
"""

import argparse
import csv
from collections import defaultdict
from pathlib import Path

import numpy as np
from PIL import Image


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='æ£€æµ‹æ•°æ®é›†ä¸­çš„æ•°æ®æ³„æ¼')
    parser.add_argument('--dataset', type=str,
                        help='æ•°æ®é›†ç›®å½•ï¼ˆå¦‚ datasets/openpartsï¼‰')
    parser.add_argument('--all', action='store_true',
                        help='æ£€æµ‹æ‰€æœ‰æ•°æ®é›†')
    parser.add_argument('--threshold', type=int, default=10,
                        help='æ±‰æ˜è·ç¦»é˜ˆå€¼ï¼ˆ0-64ï¼Œè¶Šå°è¶Šä¸¥æ ¼ï¼‰ï¼Œé»˜è®¤10')
    parser.add_argument('--hash-method', type=str, default='average',
                        choices=['average', 'difference'],
                        help='å“ˆå¸Œç®—æ³•ï¼šaverage (å¿«é€Ÿ), difference (å¹³è¡¡)')
    parser.add_argument('--output', type=str, default='results/metadata/leakage_report.csv',
                        help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--hash-size', type=int, default=8,
                        help='å“ˆå¸Œè®¡ç®—æ—¶çš„å›¾ç‰‡ç¼©æ”¾å°ºå¯¸ï¼ˆ8x8 æˆ– 16x16ï¼‰')
    return parser.parse_args()


def average_hash(image_path, hash_size=8):
    """
    è®¡ç®—å¹³å‡å“ˆå¸Œï¼ˆAverage Hash / aHashï¼‰
    
    åŸç†ï¼š
    1. ç¼©æ”¾å›¾ç‰‡åˆ° hash_size x hash_size
    2. è½¬æ¢ä¸ºç°åº¦å›¾
    3. è®¡ç®—æ‰€æœ‰åƒç´ çš„å¹³å‡å€¼
    4. æ¯ä¸ªåƒç´ å¤§äºå¹³å‡å€¼åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0
    
    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        hash_size: å“ˆå¸Œå°ºå¯¸ï¼ˆé»˜è®¤ 8ï¼Œç”Ÿæˆ 64 ä½å“ˆå¸Œï¼‰
    
    Returns:
        str: åå…­è¿›åˆ¶å“ˆå¸Œå­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        img = Image.open(image_path).convert('L')  # è½¬ç°åº¦
        img = img.resize((hash_size, hash_size), Image.Resampling.LANCZOS)
        pixels = np.array(img).flatten()
        avg = pixels.mean()
        hash_bits = (pixels > avg).astype(int)
        
        # è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
        hash_str = ''.join(str(b) for b in hash_bits)
        hash_int = int(hash_str, 2)
        return f"{hash_int:0{hash_size*hash_size//4}x}"
    except Exception as e:
        print(f"âš ï¸  è®¡ç®—å“ˆå¸Œå¤±è´¥: {image_path.name} - {e}")
        return None


def difference_hash(image_path, hash_size=8):
    """
    è®¡ç®—å·®å¼‚å“ˆå¸Œï¼ˆDifference Hash / dHashï¼‰
    
    åŸç†ï¼š
    1. ç¼©æ”¾å›¾ç‰‡åˆ° (hash_size+1) x hash_size
    2. è½¬æ¢ä¸ºç°åº¦å›¾
    3. æ¯”è¾ƒç›¸é‚»åƒç´ ï¼šå·¦è¾¹ > å³è¾¹åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0
    
    ä¼˜ç‚¹ï¼šå¯¹å›¾ç‰‡çš„å¹³ç§»å’Œç¼©æ”¾æ›´é²æ£’
    
    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        hash_size: å“ˆå¸Œå°ºå¯¸ï¼ˆé»˜è®¤ 8ï¼‰
    
    Returns:
        str: åå…­è¿›åˆ¶å“ˆå¸Œå­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        img = Image.open(image_path).convert('L')
        img = img.resize((hash_size + 1, hash_size), Image.Resampling.LANCZOS)
        pixels = np.array(img)
        
        # æ°´å¹³æ–¹å‘å·®å¼‚
        diff = pixels[:, 1:] > pixels[:, :-1]
        hash_bits = diff.flatten().astype(int)
        
        # è½¬æ¢ä¸ºåå…­è¿›åˆ¶
        hash_str = ''.join(str(b) for b in hash_bits)
        hash_int = int(hash_str, 2)
        return f"{hash_int:0{hash_size*hash_size//4}x}"
    except Exception as e:
        print(f"âš ï¸  è®¡ç®—å“ˆå¸Œå¤±è´¥: {image_path.name} - {e}")
        return None


def hamming_distance(hash1, hash2):
    """
    è®¡ç®—ä¸¤ä¸ªå“ˆå¸Œçš„æ±‰æ˜è·ç¦»ï¼ˆHamming Distanceï¼‰
    
    æ±‰æ˜è·ç¦»ï¼šä¸¤ä¸ªç­‰é•¿å­—ç¬¦ä¸²å¯¹åº”ä½ç½®ä¸åŒå­—ç¬¦çš„ä¸ªæ•°
    
    Args:
        hash1, hash2: åå…­è¿›åˆ¶å“ˆå¸Œå­—ç¬¦ä¸²
    
    Returns:
        int: æ±‰æ˜è·ç¦»ï¼ˆ0 è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼‰
    """
    if hash1 is None or hash2 is None:
        return float('inf')
    
    # è½¬æ¢ä¸ºäºŒè¿›åˆ¶å¹¶è®¡ç®—ä¸åŒä½çš„æ•°é‡
    try:
        int1 = int(hash1, 16)
        int2 = int(hash2, 16)
        xor = int1 ^ int2
        return bin(xor).count('1')
    except:
        return float('inf')


def compute_hashes(images_dir, hash_method='average', hash_size=8):
    """
    è®¡ç®—æŒ‡å®šç›®å½•æ‰€æœ‰å›¾ç‰‡çš„å“ˆå¸Œ
    
    Args:
        images_dir: å›¾ç‰‡ç›®å½•
        hash_method: å“ˆå¸Œç®—æ³•
        hash_size: å“ˆå¸Œå°ºå¯¸
    
    Returns:
        dict: {image_path: hash_value}
    """
    if not images_dir.exists():
        return {}
    
    # é€‰æ‹©å“ˆå¸Œå‡½æ•°
    hash_func = average_hash if hash_method == 'average' else difference_hash
    
    # è·å–æ‰€æœ‰å›¾ç‰‡ï¼ˆä½¿ç”¨setå»é‡ï¼Œé¿å…Windowsä¸‹å¤§å°å†™å¯¼è‡´é‡å¤ï¼‰
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
    image_files = set()  # ä½¿ç”¨setè‡ªåŠ¨å»é‡
    for ext in image_extensions:
        image_files.update(images_dir.glob(f'*{ext}'))
        image_files.update(images_dir.glob(f'*{ext.upper()}'))
    
    hashes = {}
    for img_path in image_files:
        img_hash = hash_func(img_path, hash_size)
        if img_hash:
            hashes[img_path] = img_hash
    
    return hashes


def find_duplicates(train_hashes, other_hashes, other_split, threshold=10):
    """
    æŸ¥æ‰¾è®­ç»ƒé›†å’Œå…¶ä»–é›†åˆä¹‹é—´çš„è¿‘é‡å¤å›¾åƒ
    
    Args:
        train_hashes: è®­ç»ƒé›†å“ˆå¸Œå­—å…¸
        other_hashes: å…¶ä»–é›†åˆï¼ˆval/testï¼‰å“ˆå¸Œå­—å…¸
        other_split: å…¶ä»–é›†åˆçš„åç§°ï¼ˆ'val' æˆ– 'test'ï¼‰
        threshold: æ±‰æ˜è·ç¦»é˜ˆå€¼
    
    Returns:
        list: [(train_img, other_img, distance), ...]
    """
    duplicates = []
    
    for train_path, train_hash in train_hashes.items():
        for other_path, other_hash in other_hashes.items():
            distance = hamming_distance(train_hash, other_hash)
            
            if distance <= threshold:
                duplicates.append({
                    'dataset': train_path.parts[-5],  # æ•°æ®é›†åç§°
                    'train_image': train_path.name,
                    'train_path': str(train_path),
                    'other_image': other_path.name,
                    'other_path': str(other_path),
                    'other_split': other_split,
                    'hamming_distance': distance,
                    'similarity': f"{(1 - distance / 64) * 100:.1f}%"
                })
    
    return duplicates


def generate_report(all_duplicates, output_path, threshold, hash_method):
    """
    ç”Ÿæˆæ•°æ®æ³„æ¼æŠ¥å‘Š
    
    Args:
        all_duplicates: æ‰€æœ‰æ•°æ®é›†çš„é‡å¤å›¾åƒåˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        threshold: ä½¿ç”¨çš„é˜ˆå€¼
        hash_method: ä½¿ç”¨çš„å“ˆå¸Œæ–¹æ³•
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # å†™å…¥ CSV
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        if all_duplicates:
            fieldnames = all_duplicates[0].keys()
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_duplicates)
        else:
            # å³ä½¿æ²¡æœ‰é‡å¤ï¼Œä¹Ÿåˆ›å»ºç©ºæ–‡ä»¶ï¼ˆå¸¦è¡¨å¤´ï¼‰
            writer = csv.DictWriter(f, fieldnames=[
                'dataset', 'train_image', 'train_path', 'other_image', 
                'other_path', 'other_split', 'hamming_distance', 'similarity'
            ])
            writer.writeheader()
    
    # æ·»åŠ å»ºè®®å¤„ç†ç­–ç•¥ï¼ˆä½œä¸ºæ³¨é‡Šï¼‰
    with open(output_path, 'a', encoding='utf-8') as f:
        f.write('\n# ===== æ•°æ®æ³„æ¼æ£€æµ‹æŠ¥å‘Šè¯´æ˜ =====\n')
        f.write(f'# æ£€æµ‹æ–¹æ³•: {hash_method} hash\n')
        f.write(f'# æ±‰æ˜è·ç¦»é˜ˆå€¼: {threshold} (0-64ï¼Œè¶Šå°è¶Šä¸¥æ ¼)\n')
        f.write(f'# æ£€æµ‹åˆ°çš„è¿‘é‡å¤å›¾åƒå¯¹æ•°: {len(all_duplicates)}\n')
        f.write('#\n')
        f.write('# ã€å»ºè®®å¤„ç†ç­–ç•¥ã€‘\n')
        f.write('#\n')
        f.write('# 1. å®Œå…¨ç›¸åŒï¼ˆè·ç¦»=0ï¼‰ï¼š\n')
        f.write('#    - è¿™æ˜¯ä¸¥é‡çš„æ•°æ®æ³„æ¼ï¼Œå¿…é¡»å¤„ç†\n')
        f.write('#    - å»ºè®®ï¼šä»éªŒè¯é›†/æµ‹è¯•é›†ä¸­åˆ é™¤é‡å¤å›¾åƒ\n')
        f.write('#\n')
        f.write('# 2. é«˜åº¦ç›¸ä¼¼ï¼ˆè·ç¦» 1-5ï¼‰ï¼š\n')
        f.write('#    - å¯èƒ½æ˜¯åŒä¸€åœºæ™¯çš„ä¸åŒå¸§æˆ–è½»å¾®å˜åŒ–\n')
        f.write('#    - å»ºè®®ï¼šæ‰‹åŠ¨æ£€æŸ¥ï¼Œç¡®è®¤ååˆ é™¤æˆ–ç§»åŠ¨\n')
        f.write('#\n')
        f.write('# 3. ä¸­åº¦ç›¸ä¼¼ï¼ˆè·ç¦» 6-10ï¼‰ï¼š\n')
        f.write('#    - å¯èƒ½æ˜¯ç›¸ä¼¼åœºæ™¯æˆ–ç›®æ ‡\n')
        f.write('#    - å»ºè®®ï¼šæ‰‹åŠ¨æŠ½æŸ¥ï¼Œè¯„ä¼°æ˜¯å¦å½±å“å®éªŒå…¬å¹³æ€§\n')
        f.write('#\n')
        f.write('# 4. ä½åº¦ç›¸ä¼¼ï¼ˆè·ç¦» >10ï¼‰ï¼š\n')
        f.write('#    - é€šå¸¸æ˜¯è¯¯æŠ¥æˆ–å¶ç„¶ç›¸ä¼¼\n')
        f.write('#    - å»ºè®®ï¼šå¯ä»¥ä¿ç•™ï¼Œä½†éœ€è®°å½•åœ¨å®éªŒæŠ¥å‘Šä¸­\n')
        f.write('#\n')
        f.write('# ã€å¤„ç†æ­¥éª¤ã€‘\n')
        f.write('#\n')
        f.write('# æ–¹æ¡ˆAï¼ˆæ¨èï¼‰ï¼šä»éªŒè¯é›†/æµ‹è¯•é›†ä¸­åˆ é™¤\n')
        f.write('#   1. å¤‡ä»½åŸå§‹æ•°æ®é›†\n')
        f.write('#   2. æ ¹æ® other_path åˆ—åˆ é™¤é‡å¤å›¾åƒåŠå…¶æ ‡ç­¾\n')
        f.write('#   3. é‡æ–°è¿è¡Œç»Ÿè®¡è„šæœ¬æ›´æ–° dataset_stats.json\n')
        f.write('#   4. é‡æ–°è®­ç»ƒæ¨¡å‹\n')
        f.write('#\n')
        f.write('# æ–¹æ¡ˆBï¼ˆæ›¿ä»£ï¼‰ï¼šä»è®­ç»ƒé›†ä¸­åˆ é™¤\n')
        f.write('#   1. å¦‚æœè®­ç»ƒé›†æ ·æœ¬å……è¶³ï¼Œå¯ä»¥ä»è®­ç»ƒé›†åˆ é™¤\n')
        f.write('#   2. æ ¹æ® train_path åˆ—åˆ é™¤é‡å¤å›¾åƒåŠå…¶æ ‡ç­¾\n')
        f.write('#   3. é‡æ–°è¿è¡Œç»Ÿè®¡è„šæœ¬\n')
        f.write('#\n')
        f.write('# ã€æ³¨æ„äº‹é¡¹ã€‘\n')
        f.write('#\n')
        f.write('# - åˆ é™¤æ“ä½œä¸å¯é€†ï¼ŒåŠ¡å¿…å…ˆå¤‡ä»½\n')
        f.write('# - æ„ŸçŸ¥å“ˆå¸Œå¯èƒ½å­˜åœ¨è¯¯æŠ¥ï¼Œå»ºè®®äººå·¥å¤æ ¸è·ç¦»=0çš„æƒ…å†µ\n')
        f.write('# - å¤„ç†åé‡æ–°è¿è¡Œ check_leakage.py ç¡®è®¤\n')
        f.write('# - åœ¨å®éªŒæŠ¥å‘Šä¸­è®°å½•æ‰€æœ‰å¤„ç†æ­¥éª¤\n')


def check_dataset_leakage(dataset_dir, args):
    """æ£€æµ‹å•ä¸ªæ•°æ®é›†çš„æ³„æ¼"""
    dataset_path = Path(dataset_dir)
    dataset_name = dataset_path.name
    
    print(f"\n{'='*80}")
    print(f"æ£€æµ‹æ•°æ®é›†: {dataset_name}")
    print(f"{'='*80}")
    
    # è®¡ç®—å„ä¸ªåˆ’åˆ†çš„å“ˆå¸Œ
    print(f"ğŸ“Š è®¡ç®—å›¾åƒå“ˆå¸Œ...")
    
    train_dir = dataset_path / 'images' / 'train'
    val_dir = dataset_path / 'images' / 'val'
    test_dir = dataset_path / 'images' / 'test'
    
    train_hashes = compute_hashes(train_dir, args.hash_method, args.hash_size)
    val_hashes = compute_hashes(val_dir, args.hash_method, args.hash_size)
    test_hashes = compute_hashes(test_dir, args.hash_method, args.hash_size)
    
    print(f"  train: {len(train_hashes)} å¼ ")
    print(f"  val:   {len(val_hashes)} å¼ ")
    print(f"  test:  {len(test_hashes)} å¼ ")
    
    if len(train_hashes) == 0:
        print(f"âš ï¸  è®­ç»ƒé›†ä¸ºç©ºï¼Œè·³è¿‡æ£€æµ‹")
        return []
    
    # æ£€æµ‹ train vs val
    duplicates = []
    
    if len(val_hashes) > 0:
        print(f"\nğŸ” æ£€æµ‹ train vs val...")
        train_val_dups = find_duplicates(train_hashes, val_hashes, 'val', args.threshold)
        duplicates.extend(train_val_dups)
        print(f"  å‘ç° {len(train_val_dups)} å¯¹è¿‘é‡å¤å›¾åƒ")
    
    # æ£€æµ‹ train vs test
    if len(test_hashes) > 0:
        print(f"\nğŸ” æ£€æµ‹ train vs test...")
        train_test_dups = find_duplicates(train_hashes, test_hashes, 'test', args.threshold)
        duplicates.extend(train_test_dups)
        print(f"  å‘ç° {len(train_test_dups)} å¯¹è¿‘é‡å¤å›¾åƒ")
    
    # æ‰“å°æ‘˜è¦
    if duplicates:
        identical = sum(1 for d in duplicates if d['hamming_distance'] == 0)
        high_sim = sum(1 for d in duplicates if 1 <= d['hamming_distance'] <= 5)
        medium_sim = sum(1 for d in duplicates if 6 <= d['hamming_distance'] <= 10)
        
        print(f"\nâš ï¸  æ•°æ®é›† {dataset_name} å‘ç° {len(duplicates)} å¯¹è¿‘é‡å¤å›¾åƒ")
        print(f"  å®Œå…¨ç›¸åŒ (è·ç¦»=0):   {identical:3d} å¯¹")
        print(f"  é«˜åº¦ç›¸ä¼¼ (è·ç¦»1-5):  {high_sim:3d} å¯¹")
        print(f"  ä¸­åº¦ç›¸ä¼¼ (è·ç¦»6-10): {medium_sim:3d} å¯¹")
    else:
        print(f"\nâœ… æ•°æ®é›† {dataset_name} æœªå‘ç°æ•°æ®æ³„æ¼")
    
    return duplicates


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # ç¡®å®šè¦æ£€æµ‹çš„æ•°æ®é›†åˆ—è¡¨
    datasets_to_check = []
    
    if args.all:
        # è‡ªåŠ¨å‘ç°æ‰€æœ‰æ•°æ®é›†
        datasets_root = Path('datasets')
        if datasets_root.exists():
            for subdir in datasets_root.iterdir():
                if subdir.is_dir():
                    yaml_path = subdir / 'data.yaml'
                    if yaml_path.exists():
                        datasets_to_check.append(subdir)
        
        if not datasets_to_check:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†")
            return
    
    elif args.dataset:
        dataset_path = Path(args.dataset)
        if not dataset_path.exists():
            print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_path}")
            return
        datasets_to_check.append(dataset_path)
    
    else:
        print("âŒ è¯·æŒ‡å®š --dataset æˆ– --all")
        return
    
    print("=" * 80)
    print("æ•°æ®æ³„æ¼æ£€æµ‹å·¥å…· (Data Leakage Checker)")
    print("=" * 80)
    print(f"å“ˆå¸Œæ–¹æ³•: {args.hash_method}")
    print(f"å“ˆå¸Œå°ºå¯¸: {args.hash_size}x{args.hash_size}")
    print(f"æ±‰æ˜è·ç¦»é˜ˆå€¼: {args.threshold}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"\nå°†æ£€æµ‹ {len(datasets_to_check)} ä¸ªæ•°æ®é›†:")
    for ds in datasets_to_check:
        print(f"  - {ds}")
    
    # æ£€æµ‹æ‰€æœ‰æ•°æ®é›†
    all_duplicates = []
    
    for dataset_path in datasets_to_check:
        duplicates = check_dataset_leakage(dataset_path, args)
        all_duplicates.extend(duplicates)
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\n\nğŸ“ ç”ŸæˆæŠ¥å‘Š...")
    generate_report(all_duplicates, args.output, args.threshold, args.hash_method)
    print(f"   âœ“ {args.output}")
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 80)
    if all_duplicates:
        # æŒ‰æ•°æ®é›†åˆ†ç»„ç»Ÿè®¡
        dataset_summary = defaultdict(int)
        for dup in all_duplicates:
            dataset_summary[dup['dataset']] += 1
        
        print(f"âš ï¸  æ€»è®¡å‘ç° {len(all_duplicates)} å¯¹è¿‘é‡å¤å›¾åƒ")
        print(f"\n   å„æ•°æ®é›†ç»Ÿè®¡:")
        for dataset, count in dataset_summary.items():
            print(f"     {dataset}: {count} å¯¹")
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        identical = sum(1 for d in all_duplicates if d['hamming_distance'] == 0)
        high_sim = sum(1 for d in all_duplicates if 1 <= d['hamming_distance'] <= 5)
        
        print(f"\n   ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
        print(f"     å®Œå…¨ç›¸åŒ (è·ç¦»=0):   {identical} å¯¹ {'âš ï¸  ä¸¥é‡' if identical > 0 else ''}")
        print(f"     é«˜åº¦ç›¸ä¼¼ (è·ç¦»1-5):  {high_sim} å¯¹ {'âš ï¸  éœ€å¤„ç†' if high_sim > 0 else ''}")
        
        if identical > 0:
            print(f"\n   ğŸ”´ å‘ç°å®Œå…¨ç›¸åŒçš„å›¾åƒï¼Œè¿™æ˜¯ä¸¥é‡çš„æ•°æ®æ³„æ¼ï¼")
            print(f"      è¯·ç«‹å³æŸ¥çœ‹æŠ¥å‘Šå¹¶å¤„ç†: {args.output}")
    else:
        print("âœ… æœªå‘ç°ä»»ä½•æ•°æ®æ³„æ¼ï¼æ‰€æœ‰æ•°æ®é›†åˆ’åˆ†è‰¯å¥½ã€‚")
    
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
