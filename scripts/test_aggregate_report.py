"""
æµ‹è¯•ç»“æœæ±‡æ€»å’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½

åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®å¹¶æµ‹è¯•ï¼š
1. aggregate_results.py æ•°æ®æ±‡æ€»
2. make_report.py æŠ¥å‘Šç”Ÿæˆ
"""

import sys
from pathlib import Path
import json
import shutil

project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root))


def create_mock_data():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„å®éªŒç»“æœæ•°æ®"""
    print("\n" + "="*80)
    print("åˆ›å»ºæ¨¡æ‹Ÿå®éªŒæ•°æ®")
    print("="*80)
    
    # å®šä¹‰æ¨¡æ‹Ÿå®éªŒ
    experiments = {
        'baseline': {'params': 3162272, 'gflops': 8.2},
        'ghost': {'params': 2140000, 'gflops': 5.8},
        'eca': {'params': 3165000, 'gflops': 8.3},
        'p2': {'params': 3350000, 'gflops': 9.1},
    }
    
    seeds = [0, 1, 2]
    
    # ä¸ºæ¯ä¸ªå®éªŒåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    for exp_name, exp_info in experiments.items():
        print(f"\nåˆ›å»ºå®éªŒ: {exp_name}")
        
        for seed in seeds:
            print(f"  ç§å­: {seed}")
            
            # åˆ›å»ºç›®å½•ç»“æ„
            seed_name = f"seed{seed}"
            
            # 1. è®­ç»ƒè¾“å‡ºç›®å½•
            train_dir = Path("results/runs") / exp_name / seed_name / "weights"
            train_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºç©ºæƒé‡æ–‡ä»¶ï¼ˆç”¨äºæ£€æµ‹ï¼‰
            (train_dir / "best.pt").touch()
            
            # 2. è¯„ä¼°è¾“å‡ºç›®å½• (test split)
            eval_dir = Path("results/evals") / f"{exp_name}_{seed_name}_test"
            eval_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ¨¡æ‹Ÿè¯„ä¼°æŒ‡æ ‡
            # æ·»åŠ ä¸€äº›éšæœºæ€§
            import random
            random.seed(seed + hash(exp_name))
            
            # åŸºçº¿æ€§èƒ½
            base_map50 = 0.750
            base_map5095 = 0.540
            base_ap_small = 0.320
            base_ap_medium = 0.650
            base_ap_large = 0.810
            base_center_err = 3.5
            
            # ä¸åŒå®éªŒçš„æ€§èƒ½å˜åŒ–
            if exp_name == 'baseline':
                map50 = base_map50 + random.uniform(-0.01, 0.01)
                map5095 = base_map5095 + random.uniform(-0.01, 0.01)
                ap_small = base_ap_small + random.uniform(-0.01, 0.01)
                ap_medium = base_ap_medium + random.uniform(-0.01, 0.01)
                ap_large = base_ap_large + random.uniform(-0.01, 0.01)
                center_err = base_center_err + random.uniform(-0.2, 0.2)
            elif exp_name == 'ghost':
                # Ghost: é€Ÿåº¦å¿«ï¼Œç²¾åº¦ç•¥é™
                map50 = base_map50 - 0.02 + random.uniform(-0.01, 0.01)
                map5095 = base_map5095 - 0.015 + random.uniform(-0.01, 0.01)
                ap_small = base_ap_small - 0.01 + random.uniform(-0.01, 0.01)
                ap_medium = base_ap_medium - 0.02 + random.uniform(-0.01, 0.01)
                ap_large = base_ap_large - 0.01 + random.uniform(-0.01, 0.01)
                center_err = base_center_err + 0.3 + random.uniform(-0.2, 0.2)
            elif exp_name == 'eca':
                # ECA: ç²¾åº¦æå‡ï¼Œé€Ÿåº¦ç•¥é™
                map50 = base_map50 + 0.015 + random.uniform(-0.01, 0.01)
                map5095 = base_map5095 + 0.012 + random.uniform(-0.01, 0.01)
                ap_small = base_ap_small + 0.01 + random.uniform(-0.01, 0.01)
                ap_medium = base_ap_medium + 0.015 + random.uniform(-0.01, 0.01)
                ap_large = base_ap_large + 0.012 + random.uniform(-0.01, 0.01)
                center_err = base_center_err - 0.4 + random.uniform(-0.2, 0.2)
            elif exp_name == 'p2':
                # P2: å°ç›®æ ‡æå‡ï¼Œé€Ÿåº¦é™ä½
                map50 = base_map50 + 0.010 + random.uniform(-0.01, 0.01)
                map5095 = base_map5095 + 0.008 + random.uniform(-0.01, 0.01)
                ap_small = base_ap_small + 0.05 + random.uniform(-0.01, 0.01)
                ap_medium = base_ap_medium + 0.01 + random.uniform(-0.01, 0.01)
                ap_large = base_ap_large + 0.005 + random.uniform(-0.01, 0.01)
                center_err = base_center_err - 0.2 + random.uniform(-0.2, 0.2)
            
            # ä¿å­˜æ ‡å‡†æŒ‡æ ‡
            metrics = {
                'mAP50': float(map50),
                'mAP50-95': float(map5095),
                'Precision': float(0.82 + random.uniform(-0.02, 0.02)),
                'Recall': float(0.75 + random.uniform(-0.02, 0.02)),
            }
            with open(eval_dir / "metrics.json", 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2)
            
            # ä¿å­˜å°ºåº¦æŒ‡æ ‡
            size_metrics = {
                'AP_small': float(ap_small),
                'AP_medium': float(ap_medium),
                'AP_large': float(ap_large),
            }
            with open(eval_dir / "size_metrics.json", 'w', encoding='utf-8') as f:
                json.dump(size_metrics, f, indent=2)
            
            # ä¿å­˜ä¸­å¿ƒç‚¹è¯¯å·®
            center_errors = {
                'mean_error_pixels': float(center_err),
                'median_error_pixels': float(center_err * 0.7),
                'max_error_pixels': float(center_err * 4.5),
            }
            with open(eval_dir / "center_errors.json", 'w', encoding='utf-8') as f:
                json.dump(center_errors, f, indent=2)
            
            # 3. åŸºå‡†æµ‹è¯•è¾“å‡º
            bench_dir = Path("results/benchmarks")
            bench_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•æŒ‡æ ‡
            base_fps = 82.0
            
            if exp_name == 'ghost':
                fps = base_fps * 1.25 + random.uniform(-2, 2)
            elif exp_name == 'eca':
                fps = base_fps * 0.92 + random.uniform(-2, 2)
            elif exp_name == 'p2':
                fps = base_fps * 0.85 + random.uniform(-2, 2)
            else:
                fps = base_fps + random.uniform(-2, 2)
            
            latency = 1000.0 / fps
            
            bench_metrics = {
                'model_params': exp_info['params'],
                'model_gflops': exp_info['gflops'],
                'latency_mean_ms': float(latency),
                'latency_std_ms': float(latency * 0.05),
                'latency_p50_ms': float(latency * 0.98),
                'latency_p95_ms': float(latency * 1.15),
                'latency_p99_ms': float(latency * 1.25),
                'fps': float(fps),
                'memory_allocated_mb': float(200 + random.uniform(-20, 20)),
                'memory_peak_mb': float(450 + random.uniform(-30, 30)),
            }
            
            bench_file = bench_dir / f"{exp_name}_{seed_name}_benchmark.json"
            with open(bench_file, 'w', encoding='utf-8') as f:
                json.dump(bench_metrics, f, indent=2)
    
    print("\nâœ“ æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆ")


def test_aggregate_results():
    """æµ‹è¯•ç»“æœæ±‡æ€»"""
    print("\n" + "="*80)
    print("æµ‹è¯• aggregate_results.py")
    print("="*80)
    
    import subprocess
    
    cmd = [
        sys.executable,
        "scripts/aggregate_results.py",
        "--results_dir", "results",
        "--output_dir", "results/summary",
    ]
    
    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
    
    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        encoding='utf-8',
        errors='replace'
    )
    
    print(result.stdout)
    
    if result.returncode == 0:
        print("âœ… aggregate_results.py æ‰§è¡ŒæˆåŠŸ")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        csv_file = Path("results/summary/ablation_summary.csv")
        json_file = Path("results/summary/ablation_summary.json")
        
        if csv_file.exists():
            print(f"âœ“ CSV æ–‡ä»¶å·²ç”Ÿæˆ: {csv_file}")
            print(f"  å¤§å°: {csv_file.stat().st_size} bytes")
            
            # è¯»å–å¹¶æ˜¾ç¤ºå‰å‡ è¡Œ
            with open(csv_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()[:5]
            print(f"  å‰ {len(lines)} è¡Œ:")
            for line in lines:
                print(f"    {line.rstrip()}")
        else:
            print(f"âœ— CSV æ–‡ä»¶æœªç”Ÿæˆ: {csv_file}")
            return False
        
        if json_file.exists():
            print(f"âœ“ JSON æ–‡ä»¶å·²ç”Ÿæˆ: {json_file}")
        else:
            print(f"âœ— JSON æ–‡ä»¶æœªç”Ÿæˆ: {json_file}")
            return False
        
        return True
    else:
        print(f"âŒ aggregate_results.py æ‰§è¡Œå¤±è´¥")
        print(f"stderr: {result.stderr}")
        return False


def test_make_report():
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ"""
    print("\n" + "="*80)
    print("æµ‹è¯• make_report.py")
    print("="*80)
    
    import subprocess
    
    cmd = [
        sys.executable,
        "scripts/make_report.py",
        "--summary", "results/summary/ablation_summary.json",
        "--output_dir", "results/summary",
    ]
    
    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
    
    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        encoding='utf-8',
        errors='replace'
    )
    
    print(result.stdout)
    
    if result.returncode == 0:
        print("âœ… make_report.py æ‰§è¡ŒæˆåŠŸ")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        md_file = Path("results/summary/ablation_report.md")
        plots = [
            Path("results/summary/plot_map_vs_fps.png"),
            Path("results/summary/plot_ap_small_vs_fps.png"),
            Path("results/summary/plot_center_err_vs_fps.png"),
        ]
        
        if md_file.exists():
            print(f"âœ“ Markdown æŠ¥å‘Šå·²ç”Ÿæˆ: {md_file}")
            print(f"  å¤§å°: {md_file.stat().st_size} bytes")
            
            # ç»Ÿè®¡è¡Œæ•°
            with open(md_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            print(f"  è¡Œæ•°: {len(lines)}")
        else:
            print(f"âœ— Markdown æŠ¥å‘Šæœªç”Ÿæˆ: {md_file}")
            return False
        
        # æ£€æŸ¥å›¾è¡¨
        all_plots_exist = True
        for plot_file in plots:
            if plot_file.exists():
                print(f"âœ“ å›¾è¡¨å·²ç”Ÿæˆ: {plot_file.name}")
                print(f"  å¤§å°: {plot_file.stat().st_size} bytes")
            else:
                print(f"âœ— å›¾è¡¨æœªç”Ÿæˆ: {plot_file}")
                all_plots_exist = False
        
        return all_plots_exist
    else:
        print(f"âŒ make_report.py æ‰§è¡Œå¤±è´¥")
        print(f"stderr: {result.stderr}")
        return False


def cleanup_mock_data():
    """æ¸…ç†æ¨¡æ‹Ÿæ•°æ®"""
    print("\n" + "="*80)
    print("æ¸…ç†æµ‹è¯•æ•°æ®")
    print("="*80)
    
    # å¯é€‰ï¼šä¿ç•™æ•°æ®ä¾›æŸ¥çœ‹
    response = input("æ˜¯å¦åˆ é™¤æ¨¡æ‹Ÿæ•°æ®ï¼Ÿ(y/N): ").strip().lower()
    
    if response == 'y':
        paths_to_remove = [
            Path("results/runs"),
            Path("results/evals"),
            Path("results/benchmarks"),
            Path("results/summary"),
        ]
        
        for path in paths_to_remove:
            if path.exists():
                shutil.rmtree(path)
                print(f"âœ“ å·²åˆ é™¤: {path}")
    else:
        print("ä¿ç•™æ¨¡æ‹Ÿæ•°æ®ï¼Œå¯æ‰‹åŠ¨æŸ¥çœ‹:")
        print("  - results/summary/ablation_summary.csv")
        print("  - results/summary/ablation_report.md")
        print("  - results/summary/plot_*.png")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "â•”" + "="*78 + "â•—")
    print("â•‘" + " "*20 + "ç»“æœæ±‡æ€»ä¸æŠ¥å‘Šç”Ÿæˆæµ‹è¯•" + " "*33 + "â•‘")
    print("â•š" + "="*78 + "â•")
    
    tests = [
        ("åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®", create_mock_data),
        ("æµ‹è¯• aggregate_results.py", test_aggregate_results),
        ("æµ‹è¯• make_report.py", test_make_report),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            if test_func == create_mock_data:
                # åˆ›å»ºæ•°æ®ä¸è¿”å› bool
                test_func()
                results.append((test_name, True))
            else:
                success = test_func()
                results.append((test_name, success))
        except Exception as e:
            print(f"\nâŒ {test_name} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{status}: {test_name}")
    
    print(f"\né€šè¿‡ç‡: {passed}/{total} ({100*passed/total:.0f}%)")
    
    if passed == total:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        print("  ğŸ“„ results/summary/ablation_summary.csv  - æ±‡æ€»è¡¨æ ¼")
        print("  ğŸ“„ results/summary/ablation_summary.json - å®Œæ•´æ•°æ®")
        print("  ğŸ“„ results/summary/ablation_report.md    - Markdown æŠ¥å‘Š")
        print("  ğŸ“Š results/summary/plot_map_vs_fps.png   - mAP vs FPS")
        print("  ğŸ“Š results/summary/plot_ap_small_vs_fps.png - AP_small vs FPS")
        print("  ğŸ“Š results/summary/plot_center_err_vs_fps.png - ä¸­å¿ƒè¯¯å·® vs FPS")
        print("\næŸ¥çœ‹æŠ¥å‘Š:")
        print("  cat results/summary/ablation_report.md")
        print("="*80 + "\n")
        
        # æ¸…ç†
        cleanup_mock_data()
        
        return 0
    else:
        print(f"\nâŒ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    sys.exit(main())
