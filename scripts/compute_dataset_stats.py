"""
æ•°æ®é›†ç»Ÿè®¡å·¥å…· (Dataset Statistics)

è¯¥è„šæœ¬ç»Ÿè®¡ YOLO æ ¼å¼æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
1. æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°ï¼ˆå«è¯¥ç±»åˆ«çš„å›¾ç‰‡æ•°ï¼‰
2. æ¯ä¸ªç±»åˆ«çš„æ ‡æ³¨æ¡†æ•°
3. æ ‡æ³¨æ¡†é¢ç§¯åˆ†å¸ƒï¼ˆsmall/medium/largeï¼ŒæŒ‰ COCO æ ‡å‡†ï¼‰
4. æ•°æ®é›†æ•´ä½“ç»Ÿè®¡ä¿¡æ¯

ä½¿ç”¨æ–¹æ³•:
    python scripts/compute_dataset_stats.py --dataset datasets/openparts
    python scripts/compute_dataset_stats.py --dataset datasets/selfparts
    python scripts/compute_dataset_stats.py --all  # ç»Ÿè®¡æ‰€æœ‰æ•°æ®é›†
"""

import argparse
import json
from collections import defaultdict
from pathlib import Path

import yaml


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='ç»Ÿè®¡ YOLO æ•°æ®é›†ä¿¡æ¯')
    parser.add_argument('--dataset', type=str, 
                        help='æ•°æ®é›†ç›®å½•è·¯å¾„ï¼ˆå¦‚ datasets/openpartsï¼‰')
    parser.add_argument('--all', action='store_true',
                        help='ç»Ÿè®¡æ‰€æœ‰æ•°æ®é›†')
    parser.add_argument('--output', type=str, 
                        default='results/metadata/dataset_stats.json',
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--image-size', type=int, default=640,
                        help='å‡è®¾çš„å›¾ç‰‡å°ºå¯¸ï¼ˆç”¨äºè®¡ç®—é¢ç§¯ï¼Œé»˜è®¤640ï¼‰')
    return parser.parse_args()


def load_yaml_config(yaml_path):
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    with open(yaml_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def parse_label_file(label_path):
    """
    è§£æ YOLO æ ¼å¼æ ‡ç­¾æ–‡ä»¶
    
    Args:
        label_path: æ ‡ç­¾æ–‡ä»¶è·¯å¾„
    
    Returns:
        list: [(class_id, x_center, y_center, width, height), ...]
    """
    bboxes = []
    try:
        with open(label_path, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 5:
                    class_id = int(parts[0])
                    x, y, w, h = map(float, parts[1:5])
                    bboxes.append((class_id, x, y, w, h))
    except Exception as e:
        print(f"âš ï¸  è§£ææ ‡ç­¾æ–‡ä»¶å¤±è´¥: {label_path} - {e}")
    
    return bboxes


def compute_dataset_stats(dataset_dir, config, image_size=640):
    """
    è®¡ç®—å•ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        dataset_dir: æ•°æ®é›†æ ¹ç›®å½•
        config: YAML é…ç½®
        image_size: å‡è®¾çš„å›¾ç‰‡å°ºå¯¸
    
    Returns:
        dict: ç»Ÿè®¡ä¿¡æ¯
    """
    dataset_path = Path(dataset_dir)
    class_names = config.get('names', {})
    
    # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
    if isinstance(class_names, dict):
        class_names_list = [class_names[i] for i in sorted(class_names.keys())]
    else:
        class_names_list = class_names
    
    stats = {
        'dataset_name': dataset_path.name,
        'dataset_path': str(dataset_path),
        'num_classes': config.get('nc', len(class_names_list)),
        'class_names': class_names_list,
        'splits': {},
        'class_distribution': {},
        'bbox_size_distribution': {
            'small': 0,   # area < 32^2
            'medium': 0,  # 32^2 <= area < 96^2
            'large': 0    # area >= 96^2
        },
        'total_images': 0,
        'total_bboxes': 0,
    }
    
    # åˆå§‹åŒ–ç±»åˆ«ç»Ÿè®¡
    for class_id, class_name in enumerate(class_names_list):
        stats['class_distribution'][class_name] = {
            'class_id': class_id,
            'images_count': 0,      # åŒ…å«è¯¥ç±»åˆ«çš„å›¾ç‰‡æ•°
            'bboxes_count': 0,      # è¯¥ç±»åˆ«çš„æ ‡æ³¨æ¡†æ€»æ•°
            'avg_boxes_per_image': 0.0,
        }
    
    # éå†å„ä¸ªåˆ’åˆ†
    splits_to_check = ['train', 'val', 'test']
    
    for split in splits_to_check:
        images_dir = dataset_path / 'images' / split
        labels_dir = dataset_path / 'labels' / split
        
        if not images_dir.exists():
            continue
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶ï¼ˆä½¿ç”¨setå»é‡ï¼Œé¿å…Windowsä¸‹å¤§å°å†™å¯¼è‡´é‡å¤ï¼‰
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        image_files = set()  # ä½¿ç”¨setè‡ªåŠ¨å»é‡
        for ext in image_extensions:
            image_files.update(images_dir.glob(f'*{ext}'))
            image_files.update(images_dir.glob(f'*{ext.upper()}'))
        
        image_files = list(image_files)  # è½¬å›åˆ—è¡¨
        
        split_stats = {
            'num_images': len(image_files),
            'num_bboxes': 0,
            'class_counts': defaultdict(int),
            'class_image_counts': defaultdict(int),  # è®°å½•æ¯ä¸ªç±»åˆ«å‡ºç°åœ¨å¤šå°‘å¼ å›¾ç‰‡ä¸­
        }
        
        # ç»Ÿè®¡æ¯ä¸ªå›¾ç‰‡çš„æ ‡ç­¾
        for img_path in image_files:
            label_path = labels_dir / f"{img_path.stem}.txt"
            
            if not label_path.exists():
                continue
            
            bboxes = parse_label_file(label_path)
            split_stats['num_bboxes'] += len(bboxes)
            
            # è®°å½•è¯¥å›¾ç‰‡ä¸­å‡ºç°çš„ç±»åˆ«ï¼ˆå»é‡ï¼‰
            classes_in_image = set()
            
            for class_id, x, y, w, h in bboxes:
                split_stats['class_counts'][class_id] += 1
                classes_in_image.add(class_id)
                
                # è®¡ç®—bboxé¢ç§¯ï¼ˆå½’ä¸€åŒ–åæ ‡ï¼Œéœ€ä¹˜ä»¥å›¾ç‰‡å°ºå¯¸çš„å¹³æ–¹ï¼‰
                area_pixels = w * h * image_size * image_size
                
                if area_pixels < 32 * 32:
                    stats['bbox_size_distribution']['small'] += 1
                elif area_pixels < 96 * 96:
                    stats['bbox_size_distribution']['medium'] += 1
                else:
                    stats['bbox_size_distribution']['large'] += 1
            
            # æ›´æ–°ç±»åˆ«çš„å›¾ç‰‡è®¡æ•°
            for cls_id in classes_in_image:
                split_stats['class_image_counts'][cls_id] += 1
        
        # ä¿å­˜åˆ’åˆ†ç»Ÿè®¡
        stats['splits'][split] = {
            'num_images': split_stats['num_images'],
            'num_bboxes': split_stats['num_bboxes'],
            'avg_boxes_per_image': split_stats['num_bboxes'] / split_stats['num_images'] if split_stats['num_images'] > 0 else 0,
            'class_counts': dict(split_stats['class_counts']),
            'class_image_counts': dict(split_stats['class_image_counts']),
        }
        
        stats['total_images'] += split_stats['num_images']
        stats['total_bboxes'] += split_stats['num_bboxes']
    
    # æ±‡æ€»ç±»åˆ«ç»Ÿè®¡ï¼ˆè·¨æ‰€æœ‰åˆ’åˆ†ï¼‰
    for split_name, split_data in stats['splits'].items():
        for cls_id, count in split_data['class_counts'].items():
            if cls_id < len(class_names_list):
                class_name = class_names_list[cls_id]
                stats['class_distribution'][class_name]['bboxes_count'] += count
        
        for cls_id, count in split_data['class_image_counts'].items():
            if cls_id < len(class_names_list):
                class_name = class_names_list[cls_id]
                stats['class_distribution'][class_name]['images_count'] += count
    
    # è®¡ç®—å¹³å‡å€¼
    for class_name, info in stats['class_distribution'].items():
        if info['images_count'] > 0:
            info['avg_boxes_per_image'] = round(info['bboxes_count'] / info['images_count'], 2)
    
    return stats


def print_dataset_stats(stats):
    """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯åˆ°æ§åˆ¶å°"""
    print(f"\n{'='*80}")
    print(f"æ•°æ®é›†: {stats['dataset_name']}")
    print(f"{'='*80}")
    
    print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡:")
    print(f"  æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
    print(f"  æ€»æ ‡æ³¨æ¡†æ•°: {stats['total_bboxes']}")
    print(f"  ç±»åˆ«æ•°: {stats['num_classes']}")
    if stats['total_images'] > 0:
        print(f"  å¹³å‡æ¯å¼ å›¾: {stats['total_bboxes'] / stats['total_images']:.2f} ä¸ªæ¡†")
    
    print(f"\nğŸ“‚ å„åˆ’åˆ†ç»Ÿè®¡:")
    for split in ['train', 'val', 'test']:
        if split in stats['splits']:
            split_info = stats['splits'][split]
            print(f"  {split:5s}: {split_info['num_images']:4d} å¼ å›¾ç‰‡, "
                  f"{split_info['num_bboxes']:5d} ä¸ªæ¡†, "
                  f"å¹³å‡ {split_info['avg_boxes_per_image']:.2f} æ¡†/å›¾")
    
    print(f"\nğŸ“¦ ç›®æ ‡å°ºå¯¸åˆ†å¸ƒ (COCOæ ‡å‡†):")
    size_dist = stats['bbox_size_distribution']
    total_boxes = size_dist['small'] + size_dist['medium'] + size_dist['large']
    if total_boxes > 0:
        print(f"  Small  (<32Â²px):    {size_dist['small']:5d} ({size_dist['small']/total_boxes*100:5.1f}%)")
        print(f"  Medium (32Â²-96Â²px): {size_dist['medium']:5d} ({size_dist['medium']/total_boxes*100:5.1f}%)")
        print(f"  Large  (â‰¥96Â²px):    {size_dist['large']:5d} ({size_dist['large']/total_boxes*100:5.1f}%)")
    
    print(f"\nğŸ·ï¸  ç±»åˆ«åˆ†å¸ƒ:")
    print(f"  {'ç±»åˆ«åç§°':<30} {'å›¾ç‰‡æ•°':<10} {'æ¡†æ•°':<10} {'å¹³å‡æ¡†/å›¾':<10}")
    print(f"  {'-'*65}")
    
    for class_name, info in stats['class_distribution'].items():
        if info['bboxes_count'] > 0:
            print(f"  {class_name:<30} {info['images_count']:<10} "
                  f"{info['bboxes_count']:<10} {info['avg_boxes_per_image']:<10.2f}")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†åˆ—è¡¨
    datasets_to_process = []
    
    if args.all:
        # è‡ªåŠ¨å‘ç°æ‰€æœ‰æ•°æ®é›†
        datasets_root = Path('datasets')
        if datasets_root.exists():
            for subdir in datasets_root.iterdir():
                if subdir.is_dir():
                    yaml_path = subdir / 'data.yaml'
                    if yaml_path.exists():
                        datasets_to_process.append(subdir)
        
        if not datasets_to_process:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†")
            return
    
    elif args.dataset:
        dataset_path = Path(args.dataset)
        if not dataset_path.exists():
            print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_path}")
            return
        
        yaml_path = dataset_path / 'data.yaml'
        if not yaml_path.exists():
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {yaml_path}")
            return
        
        datasets_to_process.append(dataset_path)
    
    else:
        print("âŒ è¯·æŒ‡å®š --dataset æˆ– --all")
        return
    
    print("=" * 80)
    print("æ•°æ®é›†ç»Ÿè®¡å·¥å…· (Dataset Statistics)")
    print("=" * 80)
    print(f"\nå°†ç»Ÿè®¡ {len(datasets_to_process)} ä¸ªæ•°æ®é›†:")
    for ds in datasets_to_process:
        print(f"  - {ds}")
    
    # ç»Ÿè®¡æ‰€æœ‰æ•°æ®é›†
    all_stats = {}
    
    for dataset_path in datasets_to_process:
        print(f"\n\nğŸ” æ­£åœ¨ç»Ÿè®¡: {dataset_path}...")
        
        # åŠ è½½é…ç½®
        yaml_path = dataset_path / 'data.yaml'
        config = load_yaml_config(yaml_path)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = compute_dataset_stats(dataset_path, config, args.image_size)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print_dataset_stats(stats)
        
        # ä¿å­˜åˆ°ç»“æœä¸­
        all_stats[dataset_path.name] = stats
    
    # ä¿å­˜åˆ° JSON æ–‡ä»¶
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_stats, f, indent=2, ensure_ascii=False)
    
    print(f"\n\n{'='*80}")
    print(f"âœ… ç»Ÿè®¡å®Œæˆï¼")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size} bytes")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
