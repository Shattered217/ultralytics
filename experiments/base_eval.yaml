# ============================================================================
# 基础评估配置 (Base Evaluation Configuration)
# ============================================================================
# 所有消融实验必须使用此配置文件进行评估，确保评估标准的一致性
# 
# 使用方法:
#   1. 直接使用: python val.py --cfg experiments/base_eval.yaml
#   2. 覆盖参数: python val.py --cfg experiments/base_eval.yaml --conf 0.5
#   3. 加载配置: from scripts.load_config import load_config
#
# 注意: 
#   - 评估参数与训练配置保持一致
#   - 所有实验使用相同的评估标准
#   - 确保与论文中报告的指标计算方式一致
# ============================================================================

# ----------------------------------------------------------------------------
# 数据集配置 (Dataset Configuration)
# ----------------------------------------------------------------------------
# 评估时使用与训练相同的数据集配置文件
data: datasets/openparts/data.yaml

# 备选数据集：
# data: datasets/selfparts/data.yaml

# ----------------------------------------------------------------------------
# 模型与权重配置 (Model & Weights Configuration)
# ----------------------------------------------------------------------------
# 评估时需要指定训练好的模型权重
# 通过命令行 --weights 参数指定，例如：
# --weights results/train/E0_baseline_seed0/weights/best.pt

# 模型配置（如果需要指定，否则从权重文件自动加载）
# model: ultralytics/cfg/models/v8/yolov8n.yaml

# ----------------------------------------------------------------------------
# 评估基础参数 (Evaluation Basic Parameters)
# ----------------------------------------------------------------------------
task: detect                # 任务类型：目标检测
mode: val                   # 模式：验证/评估

# 图像尺寸（必须与训练时一致）
imgsz: 640                  # 输入图像尺寸 (像素)
                            # 注意：评估时使用单一尺寸，不做多尺度测试
                            # 多尺度测试会提升性能但增加推理时间

# 批次大小（评估时可以更大，不影响结果）
batch: 16                   # 批次大小（推荐 16-32，根据显存调整）

# ----------------------------------------------------------------------------
# 后处理参数 (Post-processing Parameters)
# ----------------------------------------------------------------------------
# 这些参数直接影响评估指标，必须在所有实验中保持一致

# 置信度阈值
conf: 0.25                  # 目标置信度阈值（0.0-1.0）
                            # 推荐 0.25：平衡精确率和召回率
                            # 更低阈值：召回率高，精确率低
                            # 更高阈值：精确率高，召回率低

# IoU 阈值（NMS）
iou: 0.7                    # NMS IoU 阈值（0.0-1.0）
                            # 推荐 0.7：适合一般目标检测
                            # 更低阈值：保留更多框（可能重复）
                            # 更高阈值：过滤更多框（可能遗漏）

# 最大检测数
max_det: 300                # 每张图片最大检测目标数
                            # 推荐 300：足够处理密集场景
                            # 对于稀疏场景可以降低到 100

# ----------------------------------------------------------------------------
# 评估指标配置 (Metrics Configuration)
# ----------------------------------------------------------------------------
# COCO 评估指标（默认启用）

# mAP 计算的 IoU 阈值范围
# mAP@0.5: IoU=0.5 时的 AP
# mAP@0.5:0.95: IoU 从 0.5 到 0.95（步长 0.05）的平均 AP

# 按目标尺寸分桶的 AP（COCO 标准）
# - Small: 面积 < 32² 像素
# - Medium: 32² ≤ 面积 < 96² 像素
# - Large: 面积 ≥ 96² 像素

# ----------------------------------------------------------------------------
# 硬件配置 (Hardware Configuration)
# ----------------------------------------------------------------------------
device: 0                   # 设备：0 (GPU 0), cpu
                            # 评估推荐使用单GPU以确保一致性

workers: 2                  # 数据加载线程数（推荐 4-8）

# ----------------------------------------------------------------------------
# 输出配置 (Output Configuration)
# ----------------------------------------------------------------------------
project: results/val        # 验证结果保存目录
name: exp                   # 实验名称（建议改为具体实验ID）

# 保存选项
save_json: False            # 是否保存 COCO 格式的预测结果（用于 COCO 官方评估工具）
save_hybrid: False          # 是否保存混合版本（标签 + 预测）
save_txt: False             # 是否保存预测结果为 txt 文件
save_conf: False            # 是否在 txt 文件中保存置信度

# 可视化选项
plots: True                 # 是否生成混淆矩阵、PR 曲线、F1 曲线等图表
verbose: True               # 是否打印详细日志

# ----------------------------------------------------------------------------
# 速度测试配置 (Speed Test Configuration)
# ----------------------------------------------------------------------------
# 如果需要测试推理速度（FPS / Latency），使用以下配置：

# 速度测试模式（默认关闭）
# 开启方法：命令行添加 --speed
# 测试配置：
#   - 批次大小：1（单帧推理）
#   - 预热次数：100（避免冷启动影响）
#   - 测试次数：300（取平均值）
#   - 精度：FP32（或 FP16 如果支持）

# 示例：
# python val.py \
#     --cfg experiments/base_eval.yaml \
#     --weights results/train/E0_baseline/weights/best.pt \
#     --batch 1 \
#     --speed

# ----------------------------------------------------------------------------
# 中心点误差计算 (Center Point Error - 自定义指标)
# ----------------------------------------------------------------------------
# 中心点误差是额外的评估指标，衡量定位精度
# 计算方法：
#   error = sqrt((pred_cx - gt_cx)² + (pred_cy - gt_cy)²)
#   仅对 IoU > 0.5 的正确检测计算
# 
# 这个指标需要在评估脚本中额外实现，不在标准 YOLO 评估中包含

# ----------------------------------------------------------------------------
# 数据集划分选择 (Dataset Split Selection)
# ----------------------------------------------------------------------------
# 默认评估验证集（val split）
# 如果需要评估测试集，修改数据集配置或使用命令行参数
split: val                  # 数据划分：train, val, test

# 对于 openparts 数据集：
#   - train: 1470 张（用于训练）
#   - val: 420 张（用于验证和超参数调整）
#   - test: 210 张（用于最终评估，报告论文结果）

# 对于 selfparts 数据集：
#   - train: 399 张（用于训练）
#   - val: 100 张（用于验证，无 test 集）

# ----------------------------------------------------------------------------
# 评估最佳实践 (Evaluation Best Practices)
# ----------------------------------------------------------------------------
# 
# 1. 多次运行取平均：
#    每个实验配置使用 3 个随机种子训练，分别评估后取平均值和标准差
#
# 2. 评估划分：
#    - 验证集（val）：用于超参数调整和模型选择
#    - 测试集（test）：用于最终评估和论文报告（仅评估一次）
#
# 3. 评估时机：
#    - 训练中验证：每个 epoch 在 val 集上评估（选择 best.pt）
#    - 最终测试：所有实验完成后，在 test 集上统一评估
#
# 4. 报告指标：
#    主要指标：
#      - mAP@0.5: 主要检测性能指标
#      - mAP@0.5:0.95: 综合定位和分类性能
#      - Precision: 精确率
#      - Recall: 召回率
#    
#    按尺寸分析：
#      - AP_small: 小目标性能（对于小目标检测改进特别重要）
#      - AP_medium: 中等目标性能
#      - AP_large: 大目标性能
#    
#    效率指标：
#      - FPS: 每秒处理帧数
#      - Latency: 单帧推理时间 (ms)
#      - Params: 参数量 (M)
#      - GFLOPs: 计算量
#
# 5. 统计显著性：
#    使用 3 个种子的结果计算均值和标准差
#    报告格式：mAP@0.5 = 0.756 ± 0.003
#
# ============================================================================
# 评估命令示例 (Evaluation Command Examples)
# ============================================================================
#
# 1. 基础评估（验证集）：
#    python val.py \
#        --cfg experiments/base_eval.yaml \
#        --weights results/train/E0_baseline_seed0/weights/best.pt \
#        --data datasets/openparts/data.yaml \
#        --split val
#
# 2. 最终测试集评估：
#    python val.py \
#        --cfg experiments/base_eval.yaml \
#        --weights results/train/E0_baseline_seed0/weights/best.pt \
#        --data datasets/openparts/data.yaml \
#        --split test
#
# 3. 速度测试：
#    python val.py \
#        --cfg experiments/base_eval.yaml \
#        --weights results/train/E0_baseline_seed0/weights/best.pt \
#        --batch 1 \
#        --device 0
#
# 4. 批量评估多个种子：
#    for seed in 0 1 2; do
#        python val.py \
#            --cfg experiments/base_eval.yaml \
#            --weights results/train/E0_baseline_seed${seed}/weights/best.pt \
#            --name E0_baseline_seed${seed}_val
#    done
#
# ============================================================================
